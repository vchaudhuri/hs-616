---
title: "MCQs  from week 8-16"
author: "Vaishali Chaudhuri"
date: "March 30, 2015"
output: pdf_document
---

## Lecture 8a 

What is the difference between Principal component analysis (PCA) and exploratory Factor analysis (EFA)

* PCA is a data reduction technique that transforms a larger
number of correlated variables into a much smaller set of uncorrelated variables  and EFA is a technique to uncover the latent structure in a given set of variables
* EFA analysis accounts for a maximal amount of variance in observed variables versus PCA is linear combinations of the underlying and unique factors
* PCA and EFA are essentially the same technique of data reduction technique to find correlated patterns among different variables.
* PCA is a  technique to choose selective variables which matter in the data and compute the correlation in data only among those vraiables and EFA is a technique to extract linear relationship between all variables.

## Lecture 8b

What is the most common criteria to decide which components to be retained in PCA

* Selecting the number of components to retain by examining the eigenvalues of the k  correlation matrix among the variables 
* Selecting the number of components needed to account for some threshold cumulative amount of variance in the variables (for example, 80%) 
* Basing the number of components on existing knowledge that affects the data
* Scree plot: this is a graphical method in which you choose the factors until a break in the graph

## Lecture 9a

In the following chunk of code
```
library(ggplot2)
d=data.frame(beauty=c(1,2,6,4,4,6,7,8), intelligence=c(8,4,7,5,4,9,2,3), 
speed=c(7,6,9,5,7,6,7,8), gender=c('m','m','f','m','f','f','f','m'))
ggplot() + 
scale_size_continuous(range= c(4,12)) +
geom_point(data=d, mapping=aes(x=intelligence, y=beauty, shape=gender, 
color=gender, size=speed)) 
```
Which feature is defined by scale_size_continuous in ggplot?

* Used to define the range of point sizes to use.
* Used for marking the scale of x and y axis at continuous intervals
* Used to define degrees of aesthetic scores.
* Used to size discrete variables

## Lecture 9b

What does the corrgram package do?

* Calculates correlation of variables and displays the results graphically.
* Tabulates  a correlation matrix and plots a kernel denisty map
* Draws a boxplot of the  most correlated variables with independent variable
* Draws a heat map of the data

## Lecture 10a

What function call is used to launch a shiny app in a console?

* runApp
* read.shinyApp
* read.App like read.csv to access a .csv file
* ui.r and server.r have to be called sperately

## Lecture 10b

The five basic verbs: "filter", "select", "arrange", "mutate", "summarise",(plus group_by) are functionalities of the dplyr pckage or manipulate package or both

* dplyr
* manipulate
* both dplyr and manipulate
* neither

## Lecture 11a

What is the basic difference between hierarchial cluster vs fixed cluster analysis?

* In Hierarchial cluster plots are continually fused one-by-one in order of highest similarity or dissimilarity whereas in fixed cluster it is partition of the data, measured by the distance of the plot to the center of the cluster to which it belongs.
* In Hierarchial cluster plots are generated by partition of the data, measured by the distance of the plot to the center of the cluster to which it belongs, whereas in fixed cluster plots are continually fused one-by-one in order of highest similarity or dissimilarity.
* In Hierarchial cluster clusters are specified prior and the approach adopted is to cluster at a range of values in fixed cluster a phenomenon called chaining" is followed, where a single plot is continually added to the tail of the biggest cluster

## Lecture 11b

Finish the sentence.
In the layout function of igraph package which  treats the edges as a set of springs with spring constant set by the weights parameter to the function. Vertices with high edge weight will in general, be _______ to each other

* closer
* further  
* parallel
* equidistant

## Lecture 12a

In the following linear model
```
mod = lm(train_y ~ train_x).
```
if a list of X's(here train_x) is passed to get it's predicted Y . What would be the code in R?

* For train_x = 1, 2, and 3, use predict(mod, data.frame(train_x = c(1, 2, 3)))
* For train_x = 1, 2, and 3, use predict(mod, data.frame(train_x(1, 2, 3)))
* For train_x = 1, 2, and 3, use crossval(mod, data.frame(train_x = c(1, 2, 3)))
* For train_x = 1, 2, and 3, use coxph(mod, data.frame(train_x = c(1, 2, 3))).

## Lecture 12b

Where is the method of Least squares most useful?

* Approximate solution  in which there are more equations than unknowns
* Approximate solution  in which there are more unknowns  than equations
* Approximate solution  in which there are  equations and unknowns are equal
* Approximate solution  in which   equations is a multiples of the unknowns.

## Lecture 13a

One cannot easily tell how one variable affects the prediction using RandomForest package. How can one still predict how response will change as one changes predictor?

* By partial dependence plot 
* By partial independence plot
* By complete independence plot
* By complete dependence plot

## Lecture 13b

Simpson’s Paradox describes a situation in which variables X and Y are positively related overall, but suddenly become negatively related when conditioned on a third variable Z. What is statement is False:

* It is a forward stepwise regression, which aims to add the most important factors first.
* It is as simply being a problem arising from adding a minor factor to a model before including a major factor.
* It is similar to forward stepwise regression
* Simpson’s is not a paradox in the first place, and instead is simply the effect of using variables in the wrong order.

## Lecture 14a

What are ROCR's 3 commands to produce a simple ROC plot

* Prediction,performance, plot
* fit, prediction, plot
* fit, performance, plot
* lm,fit,plot

## Lecture 14b

What is the Bayesian Information criterion

* is a criterion for model selection among a finite set of models; the model with the lowest BIC is preferred.
* It is  a measure of the relative quality of a statistical model for a given set of data
* It offers a relative estimate of the information lost when a given model is used to represent the process that generates the data
* is a method for selecting the most appropriate model among a set of competitors for a given data set

## Lecture 15a

In Andrew Ng's lecture on Learning curves which of these statements is correct?

* A small training set results in a small training error but a large cross validation error
* Training error and cross validation error are directly proportaional to each other in a data set which fits a quadratic equation
* A small training data set leads to a large training error
* If a hypothesis has high bias increasing the training set size makes the line fit better

